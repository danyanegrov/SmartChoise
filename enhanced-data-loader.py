#!/usr/bin/env python3
"""
Enhanced data loader script for Intelligent Choice System
Loads extended database with vectorization for Milvus
"""

import asyncio
import json
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import psycopg2
from pymilvus import connections, Collection, utility, FieldSchema, CollectionSchema, DataType
import logging
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Database configuration
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'choice_db',
    'user': 'postgres',
    'password': 'password'
}

# Milvus configuration
MILVUS_CONFIG = {
    'host': 'localhost',
    'port': 19530
}

class EnhancedDataLoader:
    def __init__(self):
        self.embedder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        self.db_conn = None
        self.milvus_collection = None
        self.items_data = []
    
    def connect_databases(self):
        """Connect to PostgreSQL and Milvus"""
        try:
            # Connect to PostgreSQL
            self.db_conn = psycopg2.connect(**DB_CONFIG)
            logger.info("‚úÖ Connected to PostgreSQL")
            
            # Connect to Milvus
            connections.connect(
                alias="default",
                host=MILVUS_CONFIG['host'],
                port=MILVUS_CONFIG['port']
            )
            logger.info("‚úÖ Connected to Milvus")
            
        except Exception as e:
            logger.error(f"‚ùå Database connection failed: {e}")
            raise
    
    def setup_milvus_collection(self):
        """Setup Milvus collection for item embeddings"""
        try:
            collection_name = "item_embeddings"
            
            # Drop existing collection if exists
            if utility.has_collection(collection_name):
                utility.drop_collection(collection_name)
                logger.info("üóëÔ∏è Dropped existing collection")
            
            # Define schema
            fields = [
                FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=False),
                FieldSchema(name="item_id", dtype=DataType.INT64),
                FieldSchema(name="name", dtype=DataType.VARCHAR, max_length=200),
                FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384),
                FieldSchema(name="metadata", dtype=DataType.VARCHAR, max_length=1000)
            ]
            
            schema = CollectionSchema(
                fields=fields,
                description="Item embeddings for semantic search in intelligent choice system"
            )
            
            # Create collection
            self.milvus_collection = Collection(
                name=collection_name,
                schema=schema
            )
            
            logger.info(f"üÜï Created Milvus collection: {collection_name}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to setup Milvus collection: {e}")
            raise
    
    def load_items_from_db(self):
        """Load all items from PostgreSQL database"""
        try:
            cursor = self.db_conn.cursor()
            
            query = """
                SELECT i.id, i.name, i.description, i.price, i.rating, 
                       i.attributes, c.name as category_name
                FROM items i
                LEFT JOIN categories c ON i.category_id = c.id
                WHERE i.is_available = true
                ORDER BY i.id
            """
            
            cursor.execute(query)
            rows = cursor.fetchall()
            
            self.items_data = []
            for row in rows:
                item_id, name, description, price, rating, attributes, category_name = row
                
                self.items_data.append({
                    'id': item_id,
                    'name': name,
                    'description': description or '',
                    'price': float(price) if price else 0.0,
                    'rating': float(rating) if rating else 0.0,
                    'category_name': category_name or '–†–∞–∑–Ω–æ–µ',
                    'attributes': json.loads(attributes) if attributes else {}
                })
            
            logger.info(f"üìä Loaded {len(self.items_data)} items from database")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load items from database: {e}")
            raise
    
    def generate_embeddings(self):
        """Generate embeddings for all items"""
        try:
            logger.info("üß† Generating embeddings for all items...")
            
            # Prepare data for Milvus
            ids = []
            item_ids = []
            names = []
            categories = []
            embeddings = []
            metadata = []
            
            for i, item in enumerate(self.items_data):
                # Create comprehensive text for embedding
                text_parts = []
                
                # Add item name (most important)
                text_parts.append(item['name'])
                
                # Add category
                text_parts.append(item['category_name'])
                
                # Add description if available
                if item['description']:
                    text_parts.append(item['description'])
                
                # Add important attributes
                attrs = item['attributes']
                if 'brand' in attrs:
                    text_parts.append(f"–±—Ä–µ–Ω–¥ {attrs['brand']}")
                if 'author' in attrs:
                    text_parts.append(f"–∞–≤—Ç–æ—Ä {attrs['author']}")
                if 'genre' in attrs:
                    text_parts.append(f"–∂–∞–Ω—Ä {attrs['genre']}")
                if 'processor' in attrs:
                    text_parts.append(f"–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä {attrs['processor']}")
                if 'engine' in attrs:
                    text_parts.append(f"–¥–≤–∏–≥–∞—Ç–µ–ª—å {attrs['engine']}")
                if 'cuisine' in attrs:
                    text_parts.append(f"–∫—É—Ö–Ω—è {attrs['cuisine']}")
                
                # Combine all parts
                full_text = " ".join(text_parts)
                
                # Generate embedding
                embedding = self.embedder.encode(full_text)
                
                # Prepare data for Milvus
                ids.append(i)  # Sequential ID for Milvus
                item_ids.append(item['id'])
                names.append(item['name'][:200])  # Truncate to fit VARCHAR limit
                categories.append(item['category_name'][:100])
                embeddings.append(embedding.tolist())
                
                # Create metadata JSON
                metadata_dict = {
                    'price': item['price'],
                    'rating': item['rating'],
                    'description': item['description'][:200] if item['description'] else '',
                    'key_attributes': {k: v for k, v in attrs.items() if k in ['brand', 'author', 'genre', 'processor']}
                }
                metadata.append(json.dumps(metadata_dict, ensure_ascii=False)[:1000])
                
                if (i + 1) % 50 == 0:
                    logger.info(f"   Processed {i + 1}/{len(self.items_data)} items")
            
            # Insert into Milvus
            if embeddings:
                entities = [ids, item_ids, names, categories, embeddings, metadata]
                
                logger.info("üöÄ Inserting embeddings into Milvus...")
                insert_result = self.milvus_collection.insert(entities)
                
                # Flush to make data searchable
                self.milvus_collection.flush()
                logger.info("üíæ Flushed data to Milvus")
                
                # Create index for fast search
                logger.info("üîç Creating search index...")
                index_params = {
                    "metric_type": "COSINE",
                    "index_type": "HNSW",
                    "params": {"M": 16, "efConstruction": 200}
                }
                
                self.milvus_collection.create_index("embedding", index_params)
                logger.info("‚úÖ Created HNSW index for embeddings")
                
                # Load collection for search
                self.milvus_collection.load()
                logger.info("üéØ Collection loaded and ready for search")
                
            logger.info(f"üéâ Successfully processed {len(embeddings)} item embeddings")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to generate embeddings: {e}")
            raise
    
    def create_sample_queries(self):
        """Create sample query embeddings for testing"""
        try:
            logger.info("üìù Creating sample query embeddings...")
            
            sample_queries = [
                "–∏–≥—Ä–æ–≤–æ–π –Ω–æ—É—Ç–±—É–∫ –¥–æ 100000 —Ä—É–±–ª–µ–π",
                "–∫–Ω–∏–≥–∏ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é python",
                "—Å–µ–º–µ–π–Ω—ã–π –∞–≤—Ç–æ–º–æ–±–∏–ª—å –Ω–∞–¥–µ–∂–Ω—ã–π",
                "—Å–º–∞—Ä—Ç—Ñ–æ–Ω —Å —Ö–æ—Ä–æ—à–µ–π –∫–∞–º–µ—Ä–æ–π",
                "—Ä–µ—Å—Ç–æ—Ä–∞–Ω –∏—Ç–∞–ª—å—è–Ω—Å–∫–∞—è –∫—É—Ö–Ω—è –º–æ—Å–∫–≤–∞",
                "–æ—Ç–µ–ª—å —Ü–µ–Ω—Ç—Ä —Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥",
                "–±–µ–≥–æ–≤—ã–µ –∫—Ä–æ—Å—Å–æ–≤–∫–∏ nike adidas",
                "–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è —Ä—É—Å—Å–∫–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞",
                "–±–∏–∑–Ω–µ—Å —Å–µ–¥–∞–Ω bmw mercedes",
                "–Ω–æ—É—Ç–±—É–∫ –¥–ª—è –¥–∏–∑–∞–π–Ω–∞ –≥—Ä–∞—Ñ–∏–∫–∞"
            ]
            
            query_embeddings = []
            for query in sample_queries:
                embedding = self.embedder.encode(query)
                query_embeddings.append({
                    'query': query,
                    'embedding': embedding.tolist()
                })
            
            # Save to file for later use
            with open('data/sample/query_embeddings.json', 'w', encoding='utf-8') as f:
                json.dump(query_embeddings, f, ensure_ascii=False, indent=2)
            
            logger.info(f"üíæ Saved {len(query_embeddings)} sample query embeddings")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create sample queries: {e}")
    
    def test_vector_search(self):
        """Test vector search functionality"""
        try:
            logger.info("üß™ Testing vector search functionality...")
            
            # Test query
            test_query = "–∏–≥—Ä–æ–≤–æ–π –Ω–æ—É—Ç–±—É–∫ –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞"
            query_embedding = self.embedder.encode(test_query)
            
            # Search parameters
            search_params = {
                "metric_type": "COSINE",
                "params": {"ef": 200}
            }
            
            # Perform search
            results = self.milvus_collection.search(
                data=[query_embedding.tolist()],
                anns_field="embedding",
                param=search_params,
                limit=5,
                output_fields=["item_id", "name", "category", "metadata"]
            )
            
            logger.info(f"üîç Test query: '{test_query}'")
            logger.info("üìä Top 5 results:")
            
            for i, hit in enumerate(results[0]):
                similarity = 1 - hit.distance  # Convert distance to similarity
                logger.info(f"   {i+1}. {hit.entity.get('name')} (similarity: {similarity:.3f})")
            
            logger.info("‚úÖ Vector search test completed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Vector search test failed: {e}")
    
    def create_analytics_data(self):
        """Create additional analytics and test data"""
        try:
            logger.info("üìà Creating analytics data...")
            
            cursor = self.db_conn.cursor()
            
            # Get popular items
            cursor.execute("""
                SELECT name, COUNT(*) as interaction_count 
                FROM items i 
                JOIN user_interactions ui ON i.id = ui.item_id 
                GROUP BY i.id, name 
                ORDER BY interaction_count DESC 
                LIMIT 10
            """)
            
            popular_items = cursor.fetchall()
            
            # Get user activity stats
            cursor.execute("""
                SELECT u.username, COUNT(ui.id) as total_interactions,
                       COUNT(CASE WHEN ui.interaction_type = 'purchase' THEN 1 END) as purchases
                FROM users u 
                LEFT JOIN user_interactions ui ON u.id = ui.user_id 
                WHERE u.id > 1  -- Exclude admin
                GROUP BY u.id, u.username 
                ORDER BY total_interactions DESC
            """)
            
            user_stats = cursor.fetchall()
            
            # Create analytics summary
            analytics_data = {
                'popular_items': [{'name': item[0], 'interactions': item[1]} for item in popular_items],
                'active_users': [{'username': user[0], 'interactions': user[1], 'purchases': user[2]} for user in user_stats[:10]],
                'total_items': len(self.items_data),
                'categories_count': len(set(item['category_name'] for item in self.items_data)),
                'generation_time': datetime.now().isoformat()
            }
            
            # Save analytics
            with open('data/sample/analytics_summary.json', 'w', encoding='utf-8') as f:
                json.dump(analytics_data, f, ensure_ascii=False, indent=2)
            
            logger.info("üíæ Analytics data saved to data/sample/analytics_summary.json")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create analytics data: {e}")
    
    def run_full_pipeline(self):
        """Run the complete data loading and vectorization pipeline"""
        start_time = datetime.now()
        logger.info("üöÄ Starting Enhanced Data Loader Pipeline")
        logger.info("=" * 60)
        
        try:
            # Step 1: Connect to databases
            logger.info("1Ô∏è‚É£ Connecting to databases...")
            self.connect_databases()
            
            # Step 2: Setup Milvus collection
            logger.info("2Ô∏è‚É£ Setting up Milvus collection...")
            self.setup_milvus_collection()
            
            # Step 3: Load items from PostgreSQL
            logger.info("3Ô∏è‚É£ Loading items from PostgreSQL...")
            self.load_items_from_db()
            
            # Step 4: Generate and insert embeddings
            logger.info("4Ô∏è‚É£ Generating embeddings and indexing in Milvus...")
            self.generate_embeddings()
            
            # Step 5: Create sample query embeddings
            logger.info("5Ô∏è‚É£ Creating sample query embeddings...")
            self.create_sample_queries()
            
            # Step 6: Test vector search
            logger.info("6Ô∏è‚É£ Testing vector search functionality...")
            self.test_vector_search()
            
            # Step 7: Create analytics data
            logger.info("7Ô∏è‚É£ Creating analytics data...")
            self.create_analytics_data()
            
            # Completion
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            logger.info("=" * 60)
            logger.info("üéâ DATA LOADING PIPELINE COMPLETED SUCCESSFULLY!")
            logger.info(f"‚è±Ô∏è  Total execution time: {duration:.2f} seconds")
            logger.info("=" * 60)
            
            # Print summary
            self.print_summary()
            
        except Exception as e:
            logger.error(f"üí• Pipeline failed: {e}")
            raise
        
        finally:
            # Close connections
            if self.db_conn:
                self.db_conn.close()
                logger.info("üîå Closed database connections")
    
    def print_summary(self):
        """Print summary of loaded data"""
        logger.info("üìä DATA SUMMARY:")
        logger.info(f"   ‚Ä¢ Total items vectorized: {len(self.items_data)}")
        
        # Count by category
        category_counts = {}
        for item in self.items_data:
            cat = item['category_name']
            category_counts[cat] = category_counts.get(cat, 0) + 1
        
        logger.info("   ‚Ä¢ Items by category:")
        for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"     - {category}: {count} items")
        
        # Price statistics
        prices = [item['price'] for item in self.items_data if item['price'] > 0]
        if prices:
            logger.info(f"   ‚Ä¢ Price range: {min(prices):,.0f} - {max(prices):,.0f} ‚ÇΩ")
            logger.info(f"   ‚Ä¢ Average price: {np.mean(prices):,.0f} ‚ÇΩ")
        
        logger.info("")
        logger.info("üéØ NEXT STEPS:")
        logger.info("   1. Start the FastAPI server: uvicorn app.main:app --reload")
        logger.info("   2. Start the Streamlit frontend: streamlit run frontend/app.py")
        logger.info("   3. Test recommendations at: http://localhost:8501")

def main():
    """Main entry point"""
    try:
        # Create data directories if they don't exist
        import os
        os.makedirs('data/sample', exist_ok=True)
        
        # Run the data loader
        loader = EnhancedDataLoader()
        loader.run_full_pipeline()
        
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è  Pipeline interrupted by user")
    except Exception as e:
        logger.error(f"üí• Pipeline failed with error: {e}")
        raise

if __name__ == "__main__":
    main()